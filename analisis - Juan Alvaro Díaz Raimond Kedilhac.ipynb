{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b57f0dde",
   "metadata": {},
   "source": [
    "# Examen de programación\n",
    "\n",
    "La información teórica fue obtenida del libro *Time Series Econometrics: Learning Through Replication. Springer. (2023)* de Levendis, J. D. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "344d2052-80d9-43a0-9be6-69f4c1d17483",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pandas numpy seaborn statsmodels\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from dateparser import parse\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08bbc593",
   "metadata": {},
   "source": [
    "## Limpieza de datos\n",
    "\n",
    "El método preferente para extraer los datos de INEGI es por medio de la API usando el código token y la librería INEGIpy, sin embargo, dado que ha presentado inestabilidad, decidí descargarlas de forma individual y procesarlas con código para unirlas. \n",
    "La base de datos fue limpiada y después exportada para su uso para el script de R, cuya libreria de inegiR también estaba presentando dificultades.\n",
    "\n",
    "A pesar de estas limitaciones técnicas, toda la información utilizada en el estudio fue obtenida directamente del Banco de Información Económica (BIE), empleando los índices en cifras originales, es decir, sin ajuste estacional."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1e490e8c-23dd-4a20-bb59-89e250934675",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(file, col_name):\n",
    "    df = pd.read_csv(file, encoding='latin1')\n",
    "    df['ds'] = df.iloc[:, 0].apply(lambda x: parse(x, languages=['es']))\n",
    "    s = df.set_index('ds').sort_index()['DATA'].rename(col_name)\n",
    "    return s.dropna()\n",
    "\n",
    "\n",
    "def build_df(files, names):\n",
    "    series = [load_data(f, name) for f, name in zip(files, names)]\n",
    "    df_wide = pd.concat(series, axis=1)\n",
    "    df_long = (\n",
    "        df_wide\n",
    "        .reset_index()\n",
    "        .melt(id_vars='ds', var_name='unique_id', value_name='y')\n",
    "    )\n",
    "    return df_long"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c1e20d7-927c-4a76-b6b4-35a96d1fee7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "files = [\n",
    "    'https://raw.githubusercontent.com/jadrk040507/analisis-mifel/refs/heads/master/data/igae.csv', \n",
    "    'https://raw.githubusercontent.com/jadrk040507/analisis-mifel/refs/heads/master/data/consumo.csv', \n",
    "    'https://raw.githubusercontent.com/jadrk040507/analisis-mifel/refs/heads/master/data/inversion.csv']\n",
    "names = ['igae', 'consumo', 'inversion']\n",
    "\n",
    "df = build_df(files, names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4b2cefc-6af3-49d0-9390-15114fad65ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('data/series.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea1a4e92-caa9-4fee-8b01-d8f0878fd82e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97fe779c",
   "metadata": {},
   "source": [
    "## Análisis exporatorio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fc55fae-ecec-4c65-9530-3466a1484488",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_facets(df_long, value_col, ylabel, title):\n",
    "    ax = sns.relplot(df_long, x='ds', y=value_col, kind='line', col='unique_id')\n",
    "    ax.set_ylabels(ylabel)\n",
    "    ax.set_xlabels('')\n",
    "    ax.set_titles('{col_name}')\n",
    "    ax.fig.suptitle(title, y=1.05)\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31fee478-49b1-4b86-8485-7a465afbc5f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_facets(df, 'y', 'Índice', 'Índice base 2018')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6bd8db8-fca8-4133-8f10-ebfec81f6e8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['yoy'] = df.groupby('unique_id')['y'].pct_change(periods=12) * 100\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8da298d5-2fa0-42ca-8228-b44b95f83a9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_facets(df, 'yoy', 'Cambio porcentual anual (%)', 'Variación anual (%)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a640e475",
   "metadata": {},
   "source": [
    "## Pruebas de estacionariedad\n",
    "\n",
    "El test de ADF (Dickey-Fuller Aumentado por sus siglas en inglés) es una prueba de cointegración de la forma\n",
    "\n",
    "$$\n",
    "\\Delta y_t = \\alpha + \\gamma y_{t-1} + \\sum_{k=1}^{p-1} \\delta_k \\Delta y_{t-p} + \\varepsilon_t\n",
    "$$\n",
    "\n",
    "Donde $\\gamma = \\left(\\sum_k^p\\beta_k\\right) - 1$, por lo tanto si $\\beta \\to 1$ entonces $y_t$ es una serie de raíz unitaria. La $H_0$ es que el proceso es de raíz unitaria, mientras que $H_1$ es que la serie no es un random walk. Los términos $ \\delta_k \\Delta y_{t-p}$ se incluyen para ajustar por la autocorrelación. \n",
    "El estadístico sigue un distribución Dickey-Fuller, similar a la t-Student, pero con un ajuste en los valores críticos.\n",
    "\n",
    "\n",
    "El test KPSS (Kwiatkowski-Phillips-Schmidt-Shin) es una prueba LM (multiplicador de Lagrange) que tiene forma \n",
    "\n",
    "$$\n",
    "LM = \\frac{(\\text{score})^2}{\\text{information}} = \\frac{\\sum_{t=0}^{T}S_t^2}{\\hat{\\sigma}_\\varepsilon^2}\n",
    "$$\n",
    "\n",
    "Partimos de que el modelo se descompone en los componentes: tendencia deterministica ($\\beta t$), componente estocástico ($r_t$) y error ($\\varepsilon_t$).\n",
    "$$\n",
    "y_t = \\beta t + r_t + \\varepsilon_t\n",
    "$$\n",
    "El componente estocástico evoluciona como:\n",
    "$$\n",
    "r_t = r_{t-1} + u_t, \\quad u_t \\sim iid(0, \\sigma_u^2)\n",
    "$$\n",
    "\n",
    "La $H_0 : \\sigma_u^2 = 0$, implica que $u_t=0$ y $r_t = r_0$. El modelo se reduce a\n",
    "$$\n",
    "y_t = \\beta t + r_0 + \\varepsilon_t\n",
    "$$\n",
    "Esto es un modelo estacionario alrededor de una tendencia determinística. Bajo $H_1$ la serie $y_t$ tiene una tendencia estocástica (caminata aleatoria)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4473ff7-25ec-4137-9e76-4fb497618626",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.stattools import adfuller, kpss\n",
    "\n",
    "def adf_test(series, autolag='AIC'):\n",
    "    stat, pval, lags, nobs, crit, _ = adfuller(\n",
    "        series.dropna(),\n",
    "        autolag=autolag\n",
    "    )\n",
    "    \n",
    "    return pd.DataFrame({\n",
    "        'adf_stat': [stat],\n",
    "        'p_value': [pval],\n",
    "        'n_lags': [lags],\n",
    "        'n_obs': [nobs]\n",
    "    })\n",
    "\n",
    "\n",
    "def kpss_test(series, regression='ct'):\n",
    "    stat, pval, lags, crit = kpss(\n",
    "        series.dropna(),\n",
    "        regression=regression,\n",
    "        nlags='auto'\n",
    "    )\n",
    "    \n",
    "    return pd.DataFrame({\n",
    "        'kpss_stat': [stat],\n",
    "        'kpss_p': [pval],\n",
    "        'kpss_lags': [lags]\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d96377aa-a3d1-4b8b-9754-19afbfcd6cf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "adf_df =  df.groupby('unique_id')['y'].apply(adf_test)\n",
    "kpss_df = df.groupby('unique_id')['y'].apply(kpss_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cfb0c25-5222-42be-a108-f1be11ac388e",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(adf_df)\n",
    "display(kpss_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c18fe4a-c015-46c1-b6bd-18cec52780e2",
   "metadata": {},
   "source": [
    "La prueba de Dickey–Fuller Aumentada (ADF) fue implementada tanto en Python como en R, con diferencias en la determinación del número de rezagos. En Python, el número óptimo de rezagos se seleccionó de manera endógena utilizando el criterio de información de Akaike (AIC), el cual penaliza la inclusión de parámetros adicionales y busca un balance entre ajuste y parsimonia. Bajo esta especificación, los resultados del test ADF indican que, para todas las series, no se rechaza la hipótesis nula de presencia de raíz unitaria incluso al nivel de significancia del 1%, lo que sugiere que las series no son estacionarias en nivel.\n",
    "\n",
    "En contraste, en R la implementación del test ADF utiliza una selección de rezagos fija, lo que conduce a resultados ligeramente distintos. En particular, para las series de consumo y del IGAE se rechaza la hipótesis nula de raíz unitaria al nivel de significancia del 5%, aunque no al 1%, mientras que para la inversión no se encuentra evidencia suficiente para rechazar la hipótesis nula. Estos resultados constituyen evidencia mixta respecto a la estacionariedad en nivel de algunas series.\n",
    "\n",
    "De manera complementaria, se aplicó la prueba KPSS, cuya hipótesis nula establece que la serie es estacionaria en nivel o alrededor de una tendencia determinística. En todos los casos, el estadístico KPSS permite rechazar la hipótesis nula al nivel de significancia convencional, lo que indica la presencia de tendencias estocásticas en las series.\n",
    "\n",
    "En conjunto, considerando la sensibilidad de la prueba ADF a la especificación del número de rezagos y la evidencia robusta proporcionada por la prueba KPSS, se concluye que las series presentan comportamiento compatible con procesos integrados de orden uno. Por lo tanto, a efectos del análisis posterior, todas las series se tratan como no estacionarias en nivel."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7f8bcde",
   "metadata": {},
   "source": [
    "## Primera diferencia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f77236c-d11b-4edc-87e5-a7ce0ce1db04",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['dy'] = df.groupby('unique_id')['y'].diff()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32afb5d0-82c4-44c5-b0a2-273b807667b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_facets(df, 'dy', r'$\\Delta y$', r'$\\Delta y$, índices diferenciados')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "631473f9-2d31-4a02-b2d2-6811f871c114",
   "metadata": {},
   "outputs": [],
   "source": [
    "adf_dy =  df.groupby('unique_id')['dy'].apply(adf_test)\n",
    "kpss_dy = df.groupby('unique_id')['dy'].apply(kpss_test, regression='c')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dc1d83f-935f-4a20-b4a3-568c77ec0a91",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(adf_dy)\n",
    "display(kpss_dy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40d6522d-a6c5-40f5-97fd-0ee56f2d9bcf",
   "metadata": {},
   "source": [
    "El mismo procedimiento se aplicó a la primera diferencia de cada serie. A diferencia del análisis en niveles, los resultados del test ADF para las series diferenciadas permiten rechazar la hipótesis nula de presencia de raíz unitaria, lo que indica que las primeras diferencias son estacionarias en nivel.\n",
    "\n",
    "De manera complementaria, se aplicó la prueba KPSS a las series en primera diferencia. Dado que la diferenciación elimina cualquier componente de tendencia determinística, la prueba se especificó únicamente con constante. En todos los casos, no se rechaza la hipótesis nula de estacionariedad, lo que confirma que las series diferenciadas son estacionarias.\n",
    "\n",
    "En conjunto, los resultados de las pruebas ADF y KPSS indican que todas las series son integradas de orden uno, ya que no son estacionarias en nivel, pero sí lo son después de aplicar la primera diferencia."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "979b4b4c",
   "metadata": {},
   "source": [
    "## Pruebas de cointegración\n",
    "\n",
    "Existen diversas metodologías para realizar pruebas de cointegración. En sistemas bivariados es común utilizar el procedimiento de Engle--Granger, el cual consiste en un enfoque de dos etapas. En la primera etapa se estima la relación de largo plazo\n",
    "$$\n",
    "Y_t = \\beta' X_t + \\varepsilon_t,\n",
    "$$\n",
    "\n",
    "y en la segunda se aplica una prueba de raíz unitaria, como el test ADF, a los residuos estimados. Si los residuos resultan estacionarios, se concluye que existe cointegración entre las variables.\n",
    "\n",
    "Sin embargo, para sistemas con más de dos variables resulta más apropiado emplear el test de Johansen, el cual es un enfoque multivariado basado en un modelo de corrección de error (VECM). Este procedimiento permite determinar el número de relaciones de cointegración mediante dos pruebas distintas: el test del máximo eigenvalor y el test de la traza. Ambos son pruebas de razón de verosimilitud construidas a partir de los eigenvalores de la matriz $\\Pi$, la cual contiene toda la información sobre las relaciones de largo plazo del sistema.\n",
    "\n",
    "El modelo VECM puede escribirse como:\n",
    "$$\n",
    "\\Delta Y_t = \\sum_{i=1}^{k-1} \\Gamma_i \\Delta Y_{t-i} + \\Pi Y_{t-1} + (\\gamma + \\tau t) + \\varepsilon_t.\n",
    "$$\n",
    "\n",
    "La matriz $\\Pi$ determina la existencia de cointegración. Si el sistema contiene $n$ variables, puede haber a lo sumo $n-1$ relaciones linealmente independientes de cointegración. Cuando $\\Pi$ no es de rango completo, es decir, es singular, su determinante es igual a cero, lo cual implica que al menos uno de sus eigenvalores es igual a cero. El número de relaciones de cointegración es igual al número de eigenvalores distintos de cero de $\\Pi$.\n",
    "\n",
    "Dado que los verdaderos eigenvalores de $\\Pi$ no son observables, el test de Johansen utiliza sus estimaciones muestrales y procede de manera secuencial. En el test del máximo eigenvalor se contrasta la hipótesis nula de que el rango de $\\Pi$ es $r$ frente a la alternativa de que es $r+1$, utilizando el estadístico:\n",
    "$$\n",
    "LR(r, r+1) = -T \\ln(1 - \\lambda_{r+1}).\n",
    "$$\n",
    "El procedimiento inicia con $r=0$ y se incrementa sucesivamente hasta que no se rechaza la hipótesis nula.\n",
    "\n",
    "Por su parte, el test de la traza contrasta la hipótesis nula de que el rango de $\\Pi$ es menor o igual a $r$ frente a la alternativa de que es mayor que $r$, utilizando el estadístico:\n",
    "$$\n",
    "LR(r,n) = -T \\sum_{i=r+1}^{n} \\ln(1 - \\lambda_i).\n",
    "$$\n",
    "A diferencia del test de máximo eigenvalor, el test de la traza evalúa de manera conjunta la contribución de todos los eigenvalores restantes, permitiendo identificar el número total de relaciones de cointegración en el sistema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea5b2d05-3e28-4479-9e93-bb193a269abe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.api import VAR\n",
    "\n",
    "df_wide = df.pivot(columns='unique_id', values='y', index='ds')\n",
    "\n",
    "model = VAR(df_wide[['igae','consumo','inversion']].dropna())\n",
    "lag_order = model.select_order(12)\n",
    "lag_order.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab7e5f00-75bf-4a96-80b7-9b20a73767d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.vector_ar.vecm import coint_johansen\n",
    "\n",
    "def johansen_table(data, det_order=0, k_ar_diff=1, signif=0.05):\n",
    "    result = coint_johansen(data, det_order, k_ar_diff)\n",
    "    if signif == 0.10:\n",
    "        crit_col = 0\n",
    "    elif signif == 0.05:\n",
    "        crit_col = 1\n",
    "    elif signif == 0.01:\n",
    "        crit_col = 2\n",
    "    else:\n",
    "        raise ValueError(\"signif must be 0.10, 0.05 or 0.01\")\n",
    "\n",
    "    n = data.shape[1]\n",
    "\n",
    "    table = pd.DataFrame({\n",
    "        'r': range(n),\n",
    "        'trace_stat': result.lr1,\n",
    "        'trace_crit': result.cvt[:, crit_col],\n",
    "        'trace_reject': result.lr1 > result.cvt[:, crit_col],\n",
    "        'maxeig_stat': result.lr2,\n",
    "        'maxeig_crit': result.cvm[:, crit_col],\n",
    "        'maxeig_reject': result.lr2 > result.cvm[:, crit_col]\n",
    "    })\n",
    "\n",
    "    return table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d08516b-bbe6-4655-98e3-9ff6e4a40932",
   "metadata": {},
   "outputs": [],
   "source": [
    "johansen_table(\n",
    "    df_wide[['igae', 'consumo', 'inversion']],\n",
    "    det_order=1,\n",
    "    k_ar_diff=lag_order.aic - 1,\n",
    "    signif=0.05\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24e0c36a-c474-4c7b-b8b9-b007a3a05d9a",
   "metadata": {},
   "source": [
    "De acuerdo con los resultados obtenidos a partir de las pruebas de cointegración de Johansen, se concluye que el sistema presenta un orden de cointegración igual a 1. Tanto el test de la traza como el test del máximo eigenvalor se evalúan de manera secuencial. En ambos casos, no se rechaza la hipótesis nula de ausencia de cointegración para $r = 0$ ni para $r = 1$, mientras que la hipótesis nula es rechazada al contrastar $r = 2$. Dado que el rango máximo posible del sistema es dos, estos resultados implican la existencia de exactamente una relación de cointegración entre las variables.\n",
    "\n",
    "En consecuencia, se identifica una combinación lineal estacionaria entre el IGAE, el consumo y la inversión, lo que indica la presencia de una relación de equilibrio de largo plazo que vincula a estas variables. A pesar de que las series individuales son integradas de orden uno, comparten una tendencia común de largo plazo, consistente con la existencia de cointegración de rango uno."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82154bcc",
   "metadata": {},
   "source": [
    "## Transformación logarítmica y regresión"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff054cc8-f4b6-4092-9c5e-51da0816c838",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['ly'] = np.log(df['y'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8baac44b-d203-4043-baea-5cb869504474",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_facets(df, 'ly', 'Log', 'Series en log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b4108b2-d80b-40f8-92ab-ec1706393f7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "\n",
    "df_wide = df.pivot(columns='unique_id', index='ds', values='ly')\n",
    "\n",
    "model = (smf.ols('igae ~ consumo + inversion', df_wide)\n",
    "    .fit(\n",
    "    cov_type=\"HAC\",\n",
    "    cov_kwds={\"maxlags\": 12}\n",
    "        ))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc8d3a93-0708-4950-a18c-35dc3b6bf811",
   "metadata": {},
   "source": [
    "El modelo de regresión propuesto captura relaciones promedio entre las variables, aunque no es adecuado para realizar inferencia causal ni para fines predictivos. Bajo esta especificación en logaritmos, los coeficientes estimados se interpretan como elasticidades; es decir, miden el cambio porcentual promedio en la variable dependiente ante un cambio de $1\\%$ en la variable independiente. En particular, la estimación sugiere que un aumento de $1\\%$ en el consumo se asocia, en promedio, con un incremento aproximado de $0.74\\%$ en el IGAE, que actúa como proxy del PIB. De manera análoga, un aumento de $1\\%$ en la inversión se asocia con un incremento promedio cercano a $0.09\\%$ en el IGAE.\n",
    "\n",
    "Con el fin de corregir posibles problemas de heterocedasticidad y autocorrelación en los residuos, se emplearon errores estándar robustos de tipo HAC (Newey-West). Tras este ajuste, los coeficientes de interés resultaron estadísticamente significativos al nivel del $1\\%$, o cercanos a dicho umbral, de acuerdo con los valores $p$ obtenidos. El uso de errores HAC incrementa la incertidumbre asociada a las estimaciones al ampliar los errores estándar. Asimismo, pueden observarse ligeras discrepancias respecto a estimaciones realizadas en Python, lo cual se explica porque la librería *statsmodels* utiliza aproximaciones asintóticas para la inferencia, mientras que en R se emplea la distribución $t$ de Student, resultando en inferencia más conservadora. Recordemos que el valor $p$ representa la probabilidad, bajo la hipótesis nula, de observar un estadístico al menos tan extremo como el obtenido, y está directamente relacionado con la probabilidad de cometer un error tipo I ($\\alpha$).\n",
    "\n",
    "El estadístico $F$ rechaza la hipótesis nula de que, conjuntamente, los coeficientes de los regresores (excluyendo la constante) sean iguales a cero, lo que indica que el modelo proporciona un mejor ajuste que una especificación que solo incluya la media del logaritmo del IGAE. El coeficiente de determinación $R^2$ y su versión ajustada indican una elevada proporción de la varianza explicada, lo cual es consistente con la existencia de una relación de equilibrio de largo plazo entre las variables, dado que se ha identificado cointegración de rango uno. No obstante, este buen ajuste no debe interpretarse como evidencia de causalidad ni de capacidad predictiva fuera de muestra, ya que el modelo no incorpora explícitamente la dinámica de corto plazo ni el mecanismo de corrección de error.\n",
    "\n",
    "Finalmente, los estadísticos de Jarque-Bera y Omnibus se utilizan para evaluar la normalidad de los residuos a partir del sesgo y la curtosis. En este caso, no se rechaza la hipótesis nula de normalidad, lo que sugiere que los residuos no presentan desviaciones significativas respecto a una distribución normal.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
